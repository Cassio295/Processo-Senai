{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 - Uma estrutura de diret´rorios e nomenclatura de arquivos que permita armazenas as consultas de modo coerente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando as pastas\n",
    "import os\n",
    "\n",
    "# Lista de países (exemplo com alguns países)\n",
    "lista_paises = ['BRA', 'ARG', 'AFG', 'CHN', 'WLD']  \n",
    "anos = ['2019', '2020', '2021']\n",
    "\n",
    "# Função para criar as pastas e preenhce-las com alguns arquivos de exemplos\n",
    "def criar_estrutura(lista_paises, anos):\n",
    "    for pais_origem in lista_paises:\n",
    "        for pais_destino in lista_paises:\n",
    "            if pais_origem != pais_destino:  # Ignorar consultas inválidas\n",
    "                path = f'dados_comercio_exterior/{pais_origem}/{pais_destino}'\n",
    "                os.makedirs(path, exist_ok=True)  # Criando as pastas caso elas não existam\n",
    "                \n",
    "                for ano in anos:\n",
    "                    file_path = f'{path}/{ano}.txt'\n",
    "                    with open(file_path, 'w') as file:\n",
    "                        # Crindo os arquivos txt de exemplo das importações dos paises\n",
    "                        file.write(f'Arquivo para {pais_origem} {pais_destino} no ano {ano}\\n')  \n",
    "\n",
    "criar_estrutura(lista_paises, anos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 - Listar as etapas necessárias para a integração de 3 anos(2019-2021) de comércio exterio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'ENDPOINT DA REQUISIÇÃO'\n",
    "todos_paises = ['BRA', 'ARG','AFG'] #Em um cenário seria uma lista com todos os \n",
    "                                    #nomes dos paises ou uma planilha com essas \n",
    "                                    #informações ou uma consulta direta a um banco de dados \n",
    "parceiros = ['ARG', 'BRA', 'ARG']\n",
    "\n",
    "ano = ['2019','2020','2021']\n",
    "resultados = []\n",
    "for pais in todos_paises:\n",
    "    for parceiro in parceiros:\n",
    "        for periodo in ano:\n",
    "            response = requests.get(\n",
    "                url = url, \n",
    "                params= {\n",
    "                'pais': pais,\n",
    "                'parceiro': parceiro,\n",
    "                'ano': periodo})\n",
    "            resultados.append(response.json())\n",
    "#Essa estrutura seria responsável por fazer as requisições em todos os paises \n",
    "# passados na lista todos os paises para salvar os arquivos dentro das pastas usando\n",
    "# write primeiro eu teria que interar sobre a lista resultados pegar as informações \n",
    "# nescessárias do json que retornou e criar a lógica para salvar esses resultados nas pastas\n",
    "# ou dentro de um banco de dados caso fosse nescessário        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 - Qual/Quais arquivos(s) da sua base de dados respondem a questão: 'Qual a\n",
    "quantidade de soja o Mundo importou do Brasil em 2020?'\n",
    "A resposta irá depender da estrutura proposta em (1.1)\n",
    "R: Com base na minhas estrutura a resposta é dados_comercio_exterior\\WLD\\BRA\\2020.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4: \n",
    "Para tornar essa atualização recorrente: \n",
    "primeiro eu iria extrair o ano atual e anterior usando datetime e depois iria transformar essas informações em strigns assumindo que a requisição só aceite strings.\n",
    "\n",
    "Segundo, iria criar uma função para fazer essas requisições e salvá-las nas minhas pastas.\n",
    "\n",
    "Terceiro caso tenha um servidor que eu possa ter acesso, como se trata de um painel de dados que deve tá sempre atualizado eu compilaria meu código em um executável e usuária o agendador de tarefas do windows para que meu código rodasse todos os dias, o código em si teria tratativas de erros e retorno para de sucesso ou falha para meu e-mail, ou do meu supervisor para acompanhamento diário e criação de histórico de requisições.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considere os arquivos públicos da Receita Federal disponíveis em:\n",
    "http://200.152.38.155/CNPJ/. A página é atualizada, na média, 1 vez ao mês: os nomes\n",
    "dos arquivos são fixos.\n",
    "Suponha que você precise automatizar o processo de baixar, empilhar e manter\n",
    "atualizado os arquivos de empresas (0~9). Considere que o site é instável, portanto, o\n",
    "arquivo baixado pode estar corrompido.\n",
    "Liste as etapas necessárias para que o dataset de empresas esteja sempre atualizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R:\n",
    "Para automatizar esse processo, vou supor que eu precise fazer um processo de web scraping, para isso vou usar o selenium.\n",
    "Primeiro, abriria o site onde estão os arquivos para baixá-los, localizaria os elementos para baixar os arquivos.\n",
    "\n",
    "Segundo, especificaria o caminho para onde eu quero que os arquivos sejam salvos, após isso iria extrair os mesmos.\n",
    "\n",
    "Terceiro, iria verificar a integridade dos dados para garantir que não estão corrompidos, extrairia os mesmo e tentaria ler eles usando o pandas, para esse processo usaria um try execpt junto de um finaly para tratar esse arquvio.\n",
    "\n",
    "Quarto, supondo que os arquivos sejam, arquivos csv eu usaria o pandas para ler cada um deles e os armazenaria como um dataFrame. Supondo que cada arquivo seja de uma empresa diferente e eles tenham as mesmas colunas eu criaria um índice ou uma coluna representando o mesmo nesses DFs para representar a loja, supondo que não tenha uma coluna com o número da loja, depois eu criaria um DataFrame final contendo todas essas informações usando o método pandas.concat.\n",
    "\n",
    "Por último, criaria funções para tornar esse processo automático e mais fácil de atualizar, já que se tratando de raspagem de dados, os sites podem mudar e, com isso, os elementos dos sites também podem mudar, com isso quebrando meu código.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Escreva uma consulta SQL que retorne, para cada funcionário, o nome do\n",
    "departamento em que ele trabalhou pela primeira vez (baseado na data de início de\n",
    "trabalho), o nome do departamento onde ele está atualmente trabalhando, e a\n",
    "quantidade de departamentos diferentes em que ele já trabalhou. Use as tabelas\n",
    "employees, departments, e employee_department_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R:\n",
    "\n",
    "SELECT \n",
    "    E.EMPOYEE_NAME\n",
    "    ,D1.DEPARTMENT_NAME AS DEPARTAMENTO_INICIAL\n",
    "    ,D2.DEPARTMENT_NAME AS DEPARTAMENTO_FINAL \n",
    "    ,COUNT(DISTINCT HE.DEPARTMENT_ID) AS TOTAL_DEPARTAMENTOS\n",
    "FROM \n",
    "    EMPLOYEE E \n",
    "    JOIN employee_department_history HE ON E.EMPLOYEE_ID = HE.EMPLOYEE_ID\n",
    "    JOIN DEPARTMENTS D1 ON HE.DEPARTMENT_ID = D1.DEPARTMENT_ID\n",
    "    JOIN DEPARTMENTS D2 ON HE.DEPARTMENT_ID = D2.DEPARTMENT_ID AND HE.END_DATE IS NULL\n",
    "GROUP BY\n",
    "    E.EMPLOYEE_ID,\n",
    "    D1.DEPARTMENT_NAME,\n",
    "    D2.DEPARTMENT_NAME\n",
    "ORDER BY \n",
    "E.EMPLOYEE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. O arquivo exemplo1.parquet possui 7GB. O tempo de leitura e retorno de 1 linha específica\n",
    "são 5s - utilizando processamento paralelo (pyspark) em cluster. Contudo, você precisa criar\n",
    "uma API com recursos bem mais modestos, isto é, menor poder de processamento - mantendo\n",
    "o tempo de consulta rápido. Liste as alterações e procedimentos necessários para realizar\n",
    "isso.\n",
    "Obs: se necesserário, considere que o arquivo exemplo1.parquet atualiza 1 vez ao mês"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R:Algumas alterações que podem ser feitas para otimizar a consulta:\n",
    "Criação de indices com alguns indices criados isso pode otimizar mais o tempo de retorno da consulta,\n",
    "limitar a quantidade de linhas na saida, pré-determinar as colunas nescessárias para a saida com isso pode ser que o tempo de retorno diminua, Considerar usar banco de dados NoSQL como o MongoDB. Essas são algumas formas que podem otimizar o tempo de resposta da leitura do arquivo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Você foi instruído a criar um modelo de regressão baseado em um modelo econométrico. Ao\n",
    "terminar a implementação você verificou que o resultado está abaixo do esperado. Liste o que\n",
    "poderia ser feito para melhorar o resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R:\n",
    "Vou supor que o objetivo do modelo seja alcançar uma precisão ou medida harmônica acima de 85%.\n",
    "Caso o modelo atua não conseguiu desempenhar bem podem ser alguns motivos ele está sofrendo de overfitting (decorando os dados de treino e performando mal nos dados de teste), alguns ruídos nos dados ou ele e sensível aos Outliers (Valores discrepantes), o modelo escolhido não e o melhor para a tarefa em questão.\n",
    "Para melhorar a performasse do modelo, eu posso tentar algumas etapas. A primeira seria treinar mais de um modelo de regressão para a tarefa, como regressão linear, regressão logística, svm dentre outros, e escolher o que melhor se encaixa com os meus dados.\n",
    "Realizar o gridSearchCV juntamento com a validação cruzada para treinar meus modelos e depois usar o classification_report para verificar qual melhor se adaptou com os meus dados e verificar se ele alcançou a porcentagem desejada.\n",
    "Por último caso mesmo depois do gridSearchVC e a validação cruzada meu modelo não performou o que foi esperado, devo considerar tratar os outliers se vou normalizá-los ou remove-lós do meu conjunto de dados e passar mais tempo verificando a correlação entre meus dados e testar a hipótese de treinar os modelos sem esses dados com baixa correlação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. ELT e ETL são termos comuns na rotina de trabalho de especialistas em Soluções de\n",
    "Tecnologia da Informação, justamente por representarem estratégias de pipelines de integração\n",
    "de dados em um determinado projeto. Sobre esses procedimentos, é correto afirmar que:\n",
    " a) Ambos necessitam de uma etapa de cópia dos dados em um armazenamento\n",
    "intermediário, antes de serem transferidos para o banco de dados consolidados.\n",
    "b) Modelos locais, que utilizem dados relacionais e estruturados são ideais para estratégias\n",
    "ELT, por apresentarem menores custos e necessidades de hardware.\n",
    "c) Estratégias ETL realizam a etapa de tratamento diretamente no Banco de Dados\n",
    "Consolidado.\n",
    "d) Estrutura de dados em nuvem são ideais para a adoção de estratégias ELT, devido a\n",
    "maior rapidez no carregamento dos dados e a adequada capacidade de processamento\n",
    "posterior.\n",
    "e) Pipelines de integração do tipo ETL é indicada para casos em que a organização utilizada\n",
    "dados não-estruturados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R:A resposta correta e a letra d: Estruturas de dados em nuvem são ideias para a adoção de estratégias de ELT, devido a maior rapidez no carregamento dos dados e a adequada capacidade de processamento posterior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
